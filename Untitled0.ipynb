{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch accelerate\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"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\")\n",
        "\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "print(f\"â³ Loading {model_name} ...\")\n",
        "\n",
        "# We load the tokenizer separately to handle chat templates correctly\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "parser_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(\"âœ… Model ready for Orianâ€™s Perception layer.\")"
      ],
      "metadata": {
        "id": "irQ9icOC9CFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "#  Orian Core v56.0 | Oracle Sanitizer (Strict JSON Fact Extraction)\n",
        "# ================================================================\n",
        "import queue, re, json, os, collections, sys, copy\n",
        "\n",
        "# ---------- 0. LLM DETECTION ----------\n",
        "global_llm = None\n",
        "if 'parser_pipe' in globals():\n",
        "    try:\n",
        "        print(\"âœ… External LLM (Phi-3.5) detected.\")\n",
        "        global_llm = parser_pipe\n",
        "    except: pass\n",
        "else:\n",
        "    print(\"âš ï¸ No LLM detected. Running in Rule-Based (Regex) mode.\")\n",
        "\n",
        "# ---------- 1. INFRASTRUCTURE ----------\n",
        "class MessageBus:\n",
        "    def __init__(self): self.queues = {}\n",
        "    def register(self, name): self.queues[name] = queue.Queue()\n",
        "    def send(self, target, data):\n",
        "        if target in self.queues: self.queues[target].put(data)\n",
        "    def receive(self, name):\n",
        "        try: return self.queues[name].get_nowait()\n",
        "        except queue.Empty: return None\n",
        "\n",
        "# ---------- 2. CONTEXT MANAGER ----------\n",
        "class ContextManager:\n",
        "    def __init__(self):\n",
        "        self.last_subject = None\n",
        "        self.pronouns = [\"he\", \"she\", \"it\", \"they\", \"him\", \"her\", \"these\", \"those\"]\n",
        "    def update(self, s):\n",
        "        if s.lower() not in self.pronouns: self.last_subject = s\n",
        "    def resolve(self, text, force_subject=None):\n",
        "        subject = force_subject if force_subject else self.last_subject\n",
        "        if not subject: return text\n",
        "        words = text.split()\n",
        "        resolved = []\n",
        "        for w in words:\n",
        "            clean_w = re.sub(r'[^\\w]', '', w.lower())\n",
        "            if clean_w in self.pronouns: resolved.append(subject)\n",
        "            else: resolved.append(w)\n",
        "        return \" \".join(resolved)\n",
        "\n",
        "# ---------- 3. NORMALIZATION ----------\n",
        "class SemanticNormalizer:\n",
        "    def __init__(self):\n",
        "        self.irregulars = {\n",
        "            \"men\":\"man\",\"women\":\"woman\",\"humans\":\"human\",\"people\":\"person\",\n",
        "            \"gods\":\"god\",\"eagles\":\"eagle\",\"birds\":\"bird\",\"penguins\":\"penguin\",\n",
        "            \"wings\":\"wing\",\"flight\":\"fly\",\"capability\":\"fly\",\"abilities\":\"ability\",\n",
        "            \"airplains\":\"airplane\",\"airplanes\":\"airplane\",\"engines\":\"engine\",\n",
        "            \"animals\":\"animal\"\n",
        "        }\n",
        "        self.keep_s = {\"paris\", \"analysis\", \"physics\", \"series\", \"species\", \"lens\", \"status\", \"bus\", \"gas\", \"glass\"}\n",
        "        self.relation_map = {\n",
        "            \"is\":\"is\",\"are\":\"is\",\"was\":\"is\",\"were\":\"is\",\n",
        "            \"can\":\"can\",\"able\":\"can\",\"could\":\"can\",\"cannot\":\"cannot\",\"cant\":\"cannot\",\n",
        "            \"enables\":\"enable\",\"enable\":\"enable\",\"causes\":\"cause\",\"cause\":\"cause\",\n",
        "            \"requires\":\"require\",\"needs\":\"require\",\"push\":\"push\",\"use\":\"use\",\n",
        "            \"has\":\"has\",\"have\":\"has\",\n",
        "            \"in\":\"in\",\"inside\":\"in\",\"within\":\"in\", \"part of\":\"part of\",\n",
        "            \"is in\":\"in\", \"located in\":\"in\", \"at\":\"at\",\n",
        "            \"hate\":\"hate\", \"likes\":\"like\", \"love\":\"like\"\n",
        "        }\n",
        "    def normalize_relation(self, r):\n",
        "        return self.relation_map.get(str(r).strip().lower(), str(r).strip().lower())\n",
        "    def process_word(self, text):\n",
        "        if not text: return \"\"\n",
        "        text = re.sub(r'[^\\w\\s:_]', '', str(text).lower()).strip()\n",
        "        words = text.split()\n",
        "        out = []\n",
        "        for w in words:\n",
        "            if w in [\"a\",\"an\",\"the\",\"to\"]: continue\n",
        "            if \":\" in w: out.append(w); continue\n",
        "            if w in self.irregulars: out.append(self.irregulars[w]); continue\n",
        "            if w in self.keep_s: out.append(w); continue\n",
        "            if w.endswith(\"s\") and len(w)>3 and w not in [\"is\",\"has\",\"push\",\"pass\",\"does\",\"causes\",\"was\"]:\n",
        "                out.append(w[:-1])\n",
        "            else: out.append(w)\n",
        "        return \" \".join(out)\n",
        "    def strip_modifiers(self, text):\n",
        "        text = str(text).lower().strip()\n",
        "        mods = {}\n",
        "        match_with = re.search(r'\\bwith\\s+(\\w+)', text)\n",
        "        if match_with:\n",
        "            feat = self.process_word(match_with.group(1))\n",
        "            mods[f\"has_{feat}\"] = True\n",
        "            text = re.sub(r'\\bwith\\s+\\w+', '', text)\n",
        "        match_without = re.search(r'\\bwithout\\s+(\\w+)', text)\n",
        "        if match_without:\n",
        "            feat = self.process_word(match_without.group(1))\n",
        "            mods[f\"has_{feat}\"] = False\n",
        "            text = re.sub(r'\\bwithout\\s+\\w+', '', text)\n",
        "        return self.process_word(text), mods\n",
        "    def normalize(self, s, v, o):\n",
        "        v_lower = v.lower().strip()\n",
        "        o_lower = o.lower().strip()\n",
        "        if \"not\" in v_lower or \"dont\" in v_lower or \"dose\" in v_lower:\n",
        "            if \"have\" in v_lower or \"has\" in v_lower: v = \"not_has\"\n",
        "            elif \"is\" in v_lower or \"are\" in v_lower: v = \"is_not\"\n",
        "            elif \"can\" in v_lower: v = \"cannot\"\n",
        "            elif o_lower.startswith(\"have \"): v = \"not_has\"; o = o_lower.replace(\"have \", \"\")\n",
        "        s_base, s_mods = self.strip_modifiers(s)\n",
        "        o_base, o_mods = self.strip_modifiers(o)\n",
        "        v_norm = self.normalize_relation(v)\n",
        "        if v_norm in [\"fly\",\"swim\",\"walk\",\"run\"] and not o_base:\n",
        "            o_base = v_norm; v_norm = \"can\"\n",
        "        mods = {**s_mods, **o_mods}\n",
        "        return s_base, v_norm, o_base, mods\n",
        "\n",
        "# ---------- 4. PARSER ----------\n",
        "class HybridParser:\n",
        "    def __init__(self, llm):\n",
        "        self.pipe = llm\n",
        "        self.norm = SemanticNormalizer()\n",
        "    def heuristic_split(self, text):\n",
        "        text_clean = text.lower().strip('.')\n",
        "        if \" to \" in text_clean and any(x in text_clean for x in [\"push\", \"use\", \"pull\"]):\n",
        "            parts = text_clean.split(\" to \")\n",
        "            cause = parts[0].strip(); effect = parts[1].strip()\n",
        "            triples = self.basic_parse(cause)\n",
        "            out = []\n",
        "            for s, v, o in triples:\n",
        "                out.append([s, v, o])\n",
        "                sn, vn, on, _ = self.norm.normalize(s, v, o)\n",
        "                bridge = f\"act:{vn}_{on.replace(' ', '_')}\"\n",
        "                out.append([bridge, \"enable\", effect])\n",
        "            return out\n",
        "        return self.basic_parse(text)\n",
        "    def basic_parse(self, text):\n",
        "        triples = []\n",
        "        pattern = r\"(.*?)\\s+(is not|is in|is at|is inside|located in|part of|is|are|can|cannot|cant|does not|dose not|dont|has|have|enable|enables|cause|causes|fly|require|requires|push|pushes|in|inside|at|likes|like|hate|hates)\\s*(.*)\"\n",
        "        match = re.search(pattern, text.lower().strip())\n",
        "        if match:\n",
        "            triples.append([match.group(1), match.group(2), match.group(3)])\n",
        "        elif len(text.split()) == 2:\n",
        "            triples.append([text.split()[0], text.split()[1], \"\"])\n",
        "        return triples\n",
        "    def parse_sentence(self, sent):\n",
        "        triples = []\n",
        "        raw_triples = self.heuristic_split(sent)\n",
        "        for s, v, o in raw_triples:\n",
        "            sn, vn, on, _ = self.norm.normalize(s, v, o)\n",
        "            if sn and vn: triples.append([sn, vn, on])\n",
        "        return {\"triples\": triples, \"text\": sent}\n",
        "\n",
        "# ---------- 5. MEMORY (System 1) ----------\n",
        "class System1:\n",
        "    def __init__(self, bus, ctx_mgr, filename=\"orian_brain.json\"):\n",
        "        self.bus = bus; self.bus.register(\"system1\")\n",
        "        self.filename = filename\n",
        "        self.ctx = ctx_mgr\n",
        "        self.graph = self.load_memory()\n",
        "        self.mutable_relations = [\"in\", \"at\", \"near\", \"on\", \"feeling\", \"status\", \"doing\"]\n",
        "\n",
        "    def seed_basic_knowledge(self):\n",
        "        seeds = [\n",
        "            [\"bird\", \"can\", \"fly\"],\n",
        "            [\"fish\", \"can\", \"swim\"],\n",
        "            [\"human\", \"is\", \"mortal\"],\n",
        "            [\"dog\", \"can\", \"bark\"],\n",
        "            [\"penguin\", \"is\", \"bird\"],\n",
        "            [\"penguin\", \"cannot\", \"fly\"]\n",
        "        ]\n",
        "        print(\"  [System] Seeding basic knowledge...\")\n",
        "        self.learn({\"triples\": seeds}, force=True)\n",
        "\n",
        "    def load_memory(self):\n",
        "        if os.path.exists(self.filename):\n",
        "            try:\n",
        "                with open(self.filename, 'r') as f:\n",
        "                    d = json.load(f)\n",
        "                    print(f\"  [System] Loaded memory from {self.filename}\")\n",
        "                    return collections.defaultdict(lambda: collections.defaultdict(list),\n",
        "                        {k: collections.defaultdict(list, v) for k, v in d.items()})\n",
        "            except: pass\n",
        "        return collections.defaultdict(lambda: collections.defaultdict(list))\n",
        "\n",
        "    def save_memory(self):\n",
        "        with open(self.filename, 'w') as f: json.dump(self.graph, f, indent=2)\n",
        "\n",
        "    def detect_conflict(self, s, v, o):\n",
        "        if s not in self.graph: return False, None\n",
        "        if v == \"is\" and \"is\" in self.graph[s]:\n",
        "            existing = self.graph[s][\"is\"][0]\n",
        "            if existing != o: return True, f\"is {existing}\"\n",
        "        if v in self.mutable_relations and v in self.graph[s]:\n",
        "            return \"update\", self.graph[s][v][0]\n",
        "        if v == \"can\" and \"cannot\" in self.graph[s] and o in self.graph[s][\"cannot\"]: return True, f\"cannot {o}\"\n",
        "        if v == \"cannot\" and \"can\" in self.graph[s] and o in self.graph[s][\"can\"]: return True, f\"can {o}\"\n",
        "        return False, None\n",
        "\n",
        "    def learn(self, msg, force=False, graph_target=None):\n",
        "        target_graph = graph_target if graph_target is not None else self.graph\n",
        "        is_simulation = graph_target is not None\n",
        "\n",
        "        for s, v, o in msg[\"triples\"]:\n",
        "            if not is_simulation: self.ctx.update(s)\n",
        "            if not force and not is_simulation:\n",
        "                conflict, old_val = self.detect_conflict(s, v, o)\n",
        "                if conflict == \"update\":\n",
        "                    if old_val in target_graph[s][v]: target_graph[s][v].remove(old_val)\n",
        "                    print(f\"  [Memory] State Change: {s} was {v} {old_val} -> now {v} {o}\")\n",
        "                    target_graph[s][v].append(o)\n",
        "                    self.save_memory()\n",
        "                    continue\n",
        "                if conflict is True:\n",
        "                    self.bus.send(\"system3\", {\"type\": \"conflict\", \"s\": s, \"v\": v, \"old\": old_val, \"new\": o})\n",
        "                    return\n",
        "            if o not in target_graph[s][v]:\n",
        "                target_graph[s][v].append(o)\n",
        "                if not is_simulation:\n",
        "                    print(f\"  [Memory] Learned: {s} --[{v}]--> {o}\")\n",
        "                    self.save_memory()\n",
        "                if v == \"is\" and o in target_graph:\n",
        "                    for attr in target_graph[o].get(\"has\", []):\n",
        "                        if attr not in target_graph[s][\"has\"]:\n",
        "                            target_graph[s][\"has\"].append(attr)\n",
        "\n",
        "    def forget(self, s, v, o):\n",
        "        if s in self.graph and v in self.graph[s]:\n",
        "            if o in self.graph[s][v]: self.graph[s][v].remove(o)\n",
        "            self.save_memory()\n",
        "\n",
        "    def process(self):\n",
        "        msg = self.bus.receive(\"system1\")\n",
        "        if msg: self.learn(msg)\n",
        "\n",
        "# ---------- 6. REASONER (System 2 + Oracle Sanitizer) ----------\n",
        "class System2:\n",
        "    def __init__(self, bus, parser_ref, sys1_ref, llm):\n",
        "        self.bus = bus; self.bus.register(\"system2\")\n",
        "        self.norm = SemanticNormalizer()\n",
        "        self.parser = parser_ref\n",
        "        self.sys1 = sys1_ref\n",
        "        self.llm = llm\n",
        "\n",
        "    def get_ancestors(self, subject, graph):\n",
        "        ancestors = []\n",
        "        q = [(subject, 0)]; visited = set()\n",
        "        while q:\n",
        "            curr, dist = q.pop(0)\n",
        "            if curr in visited: continue\n",
        "            visited.add(curr)\n",
        "            if \"is\" in graph[curr]:\n",
        "                for parent in graph[curr][\"is\"]:\n",
        "                    ancestors.append((parent, dist + 1))\n",
        "                    q.append((parent, dist + 1))\n",
        "        return ancestors\n",
        "\n",
        "    def check_transitive(self, start, relation, target, graph):\n",
        "        q = [[start]]; visited = set()\n",
        "        while q:\n",
        "            path = q.pop(0); curr = path[-1]\n",
        "            if len(path) > 8: continue\n",
        "            if curr in visited: continue\n",
        "            visited.add(curr)\n",
        "            if curr not in graph: continue\n",
        "            if relation in graph[curr]:\n",
        "                if target in graph[curr][relation]: return path + [relation, target]\n",
        "                for neighbor in graph[curr][relation]:\n",
        "                    if neighbor not in path: q.append(path + [relation, neighbor])\n",
        "        return None\n",
        "\n",
        "    def trace_causality(self, start, target, graph):\n",
        "        q = [[start]]; visited = set()\n",
        "        while q:\n",
        "            path = q.pop(0); curr = path[-1]\n",
        "            if len(path) > 8 or curr in visited: continue\n",
        "            visited.add(curr)\n",
        "            if target in graph[curr].get(\"enable\", []): return path + [\"enable\", target]\n",
        "            for rel in [\"push\", \"pull\", \"use\", \"cause\"]:\n",
        "                if rel in graph[curr]:\n",
        "                    for obj in graph[curr][rel]:\n",
        "                        bridge = f\"act:{rel}_{obj.replace(' ', '_')}\"\n",
        "                        if bridge in graph and target in graph[bridge].get(\"enable\", []):\n",
        "                            return path + [rel, obj, \"(bridge)\", \"enable\", target]\n",
        "        ancestors = self.get_ancestors(start, graph)\n",
        "        for anc, _ in ancestors:\n",
        "            res = self.trace_causality(anc, target, graph)\n",
        "            if res: return [start, \"is\", anc] + res\n",
        "        return None\n",
        "\n",
        "    def format_proof(self, s, path, graph, original_target, is_transitive=False):\n",
        "        steps = []\n",
        "        i = 0\n",
        "        step_inc = 2 if is_transitive else 3\n",
        "        while i < len(path):\n",
        "            word = path[i]\n",
        "            if not is_transitive and word == \"(bridge)\":\n",
        "                i += 1; continue\n",
        "            if i+1 >= len(path): break\n",
        "            verb = path[i+1]; obj = path[i+2] if i+2 < len(path) else original_target\n",
        "            if is_transitive:\n",
        "                if verb == \"is\": steps.append(f\"{word} is {obj}.\")\n",
        "                else: steps.append(f\"{word} is {verb} {obj}.\")\n",
        "            else:\n",
        "                if verb == \"enable\": steps.append(f\"This action enables {obj}.\")\n",
        "                elif verb == \"is\": steps.append(f\"{word} is a {obj}.\")\n",
        "                else: steps.append(f\"{word} {verb} {obj}.\")\n",
        "            i += step_inc\n",
        "        conf = max(10, 100 - ((len(steps)//2) * 5))\n",
        "        return f\"Yes. \" + \" \".join(steps) + f\" (Confidence: {conf}%)\"\n",
        "\n",
        "    def solve_bool(self, s, v, o, graph):\n",
        "        if s not in graph: return None\n",
        "        if o in graph[s].get(v, []): return f\"Yes. I know this directly: {s} {v} {o}. (Confidence: 100%)\"\n",
        "        if v in [\"is\", \"in\", \"part of\", \"inside\", \"located in\"]:\n",
        "            chain = self.check_transitive(s, v, o, graph)\n",
        "            if chain: return self.format_proof(s, chain, graph, o, is_transitive=True)\n",
        "        neg_v = \"cannot\" if v == \"can\" else \"not_has\"\n",
        "        if neg_v in graph[s] and o in graph[s][neg_v]: return f\"No. {s} explicitly {neg_v} {o}. (Confidence: 100%)\"\n",
        "        ancestors = self.get_ancestors(s, graph)\n",
        "        for anc, dist in ancestors:\n",
        "            if neg_v in graph[anc] and o in graph[anc][neg_v]:\n",
        "                 return f\"No. {s} is a {anc}, and {anc} {neg_v} {o}. (Confidence: 90%)\"\n",
        "            if o in graph[anc].get(v, []):\n",
        "                return f\"Yes. {s} is a {anc}, and {anc} {v} {o}. (Confidence: 90%)\"\n",
        "        return None\n",
        "\n",
        "    def consult_oracle(self, text):\n",
        "        if not self.llm: return \"I don't have enough evidence (and no Oracle available).\"\n",
        "\n",
        "        print(\"  [System 2] Consulting Oracle (Strict JSON Extraction)...\")\n",
        "        # THE SANITIZATION PROMPT\n",
        "        prompt = (\n",
        "            f\"<|user|>Extract the main fact as JSON {{'s': subject, 'v': verb, 'o': object}}. \"\n",
        "            f\"If uncertain, return {{}}. \"\n",
        "            f\"Example: 'Cats hate water' -> {{'s': 'cat', 'v': 'hate', 'o': 'water'}}.\\n\"\n",
        "            f\"Question: {text}\\n<|assistant|>\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            out = self.llm(prompt, max_new_tokens=40, return_full_text=False)\n",
        "            raw = out[0][\"generated_text\"]\n",
        "            start = raw.find('{')\n",
        "            end = raw.rfind('}') + 1\n",
        "            if start != -1 and end != -1:\n",
        "                j = json.loads(raw[start:end])\n",
        "                if j and 's' in j and 'v' in j and 'o' in j:\n",
        "                    # Normalize before learning\n",
        "                    s, v, o, _ = self.norm.normalize(j['s'], j['v'], j['o'])\n",
        "\n",
        "                    # Validate that it's a real fact, not a sentence\n",
        "                    if len(s.split()) < 4 and len(o.split()) < 6:\n",
        "                        self.sys1.learn({\"triples\": [[s, v, o]]})\n",
        "                        return f\"I just learned that: {s} {v} {o}. So, yes.\"\n",
        "\n",
        "            return \"The Oracle's answer was too vague to trust.\"\n",
        "        except Exception as e:\n",
        "            return f\"Oracle failed: {e}\"\n",
        "\n",
        "    def respond(self, text, graph):\n",
        "        text = text.lower().strip()\n",
        "        if \"how\" in text:\n",
        "            m = re.search(r\"how (?:do|does) (.*?) (.*)\", text)\n",
        "            if m:\n",
        "                s_raw, a_raw = m.groups()\n",
        "                s, _, a, _ = self.norm.normalize(s_raw, \"can\", a_raw.replace(\"?\",\"\"))\n",
        "                proof = self.trace_causality(s, a, graph)\n",
        "                if proof: return self.format_proof(s, proof, graph, a, is_transitive=False)\n",
        "                return f\"I cannot trace the mechanism for {s} -> {a}.\"\n",
        "\n",
        "        m_q = re.search(r\"^(can|could|would|do|does|is|are)\\s+(.*?)\\s+(.*?)\\??$\", text)\n",
        "        if m_q:\n",
        "            aux, s_raw, remainder = m_q.groups()\n",
        "            remainder = remainder.replace(\"?\",\"\").strip()\n",
        "            v = \"can\"; o = remainder\n",
        "            for rel in [\"in \", \"inside \", \"part of \", \"at \"]:\n",
        "                if remainder.startswith(rel):\n",
        "                    v = rel.strip(); o = remainder[len(rel):].strip(); break\n",
        "            else:\n",
        "                if \" \" not in remainder and remainder not in [\"mortal\", \"human\", \"god\", \"bird\"]: v=\"can\"\n",
        "                elif remainder in [\"mortal\", \"human\", \"god\", \"bird\"]: v=\"is\"\n",
        "                elif aux in [\"is\",\"are\"]: v=\"is\"\n",
        "            s_norm, v_norm, o_norm, _ = self.norm.normalize(s_raw, v, o)\n",
        "\n",
        "            ans = self.solve_bool(s_norm, v_norm, o_norm, graph)\n",
        "            if ans: return ans\n",
        "\n",
        "            return self.consult_oracle(text)\n",
        "\n",
        "        return \"Unknown question format.\"\n",
        "\n",
        "# ---------- 7. SYSTEM 3 ----------\n",
        "class System3:\n",
        "    def __init__(self, bus, sys1):\n",
        "        self.bus = bus; self.bus.register(\"system3\")\n",
        "        self.sys1 = sys1\n",
        "        self.active_conflict = None\n",
        "    def check_conflict(self):\n",
        "        msg = self.bus.receive(\"system3\")\n",
        "        if msg and msg[\"type\"] == \"conflict\":\n",
        "            self.active_conflict = msg\n",
        "            print(\"\\n\" + \"!\"*50)\n",
        "            print(\" ðŸ›‘ CONFLICT DETECTED\")\n",
        "            print(f\"  Known: {msg['s']} -- {msg['old']}\")\n",
        "            print(f\"  New:   {msg['s']} -- {msg['v']} {msg['new']}\")\n",
        "            print(\"  Type 'Old', 'New', or 'Both'.\")\n",
        "            print(\"!\"*50 + \"\\n\")\n",
        "            return True\n",
        "        return False\n",
        "    def resolve(self, choice):\n",
        "        ctx = self.active_conflict\n",
        "        if \"new\" in choice.lower():\n",
        "            old_clean = ctx['old'].replace(\"is \", \"\")\n",
        "            self.sys1.forget(ctx['s'], ctx['v'], old_clean)\n",
        "            self.sys1.learn({\"triples\": [[ctx['s'], ctx['v'], ctx['new']]]}, force=True)\n",
        "            print(\"  [System 3] Overwritten.\")\n",
        "        elif \"both\" in choice.lower():\n",
        "            self.sys1.learn({\"triples\": [[ctx['s'], ctx['v'], ctx['new']]]}, force=True)\n",
        "            print(\"  [System 3] Both Accepted.\")\n",
        "        else:\n",
        "            print(\"  [System 3] Kept Original.\")\n",
        "        self.active_conflict = None\n",
        "\n",
        "# ---------- 8. MAIN ----------\n",
        "class OrianCore:\n",
        "    def __init__(self, llm):\n",
        "        self.bus = MessageBus()\n",
        "        self.ctx = ContextManager()\n",
        "        self.parser = HybridParser(llm)\n",
        "        self.sys1 = System1(self.bus, self.ctx)\n",
        "        self.sys2 = System2(self.bus, self.parser, self.sys1, llm)\n",
        "        self.sys3 = System3(self.bus, self.sys1)\n",
        "        if not os.path.exists(self.sys1.filename):\n",
        "            self.sys1.seed_basic_knowledge()\n",
        "    def run_simulation(self, condition, query):\n",
        "        print(f\"\\nðŸŒŒ ENTERING DREAM WORLD: 'If {condition}, {query}?'\")\n",
        "        sim_graph = copy.deepcopy(self.sys1.graph)\n",
        "        parsed_cond = self.parser.parse_sentence(condition)\n",
        "        sim_subject = None\n",
        "        if parsed_cond[\"triples\"]:\n",
        "            sim_subject = parsed_cond[\"triples\"][0][0]\n",
        "            self.sys1.learn(parsed_cond, force=True, graph_target=sim_graph)\n",
        "        resolved_query = self.ctx.resolve(query, force_subject=sim_subject)\n",
        "        if resolved_query != query: print(f\"    (Resolved query to: '{resolved_query}')\")\n",
        "        response = self.sys2.respond(resolved_query, sim_graph)\n",
        "        print(f\"ðŸ§  SIMULATION RESULT: {response}\")\n",
        "        print(\"ðŸ’¥ DREAM WORLD DESTROYED.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    O = OrianCore(global_llm)\n",
        "    print(\"\\nðŸ¤– ORIAN v56.0 (Oracle Sanitizer)\\n\")\n",
        "    while True:\n",
        "        try:\n",
        "            sys.stdout.flush()\n",
        "            if O.sys3.active_conflict:\n",
        "                u = input(\"Resolution: \")\n",
        "                O.sys3.resolve(u)\n",
        "                continue\n",
        "            u = input(\"\\nUser: \")\n",
        "            if u.lower() in [\"quit\",\"exit\"]: break\n",
        "            if not u: continue\n",
        "            if u.lower().startswith(\"if \") and \",\" in u:\n",
        "                parts = u.split(\",\", 1)\n",
        "                O.run_simulation(parts[0][3:].strip(), parts[1].strip())\n",
        "                continue\n",
        "            sentences = re.split(r'(?<=[.?!])\\s+', u)\n",
        "            for sent in sentences:\n",
        "                sent = sent.strip()\n",
        "                if not sent: continue\n",
        "                resolved_sent = O.ctx.resolve(sent)\n",
        "                if \"?\" in sent:\n",
        "                    print(f\"{O.sys2.respond(resolved_sent, O.sys1.graph)}\")\n",
        "                else:\n",
        "                    parsed = O.parser.parse_sentence(resolved_sent)\n",
        "                    if parsed[\"triples\"]:\n",
        "                        O.bus.send(\"system1\", parsed)\n",
        "                        O.sys1.process()\n",
        "                        if O.sys3.check_conflict(): break\n",
        "        except Exception as e: print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl_xkwnwQdPp",
        "outputId": "acdaa87f-3c49-4c1f-e416-e67352f5f2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… External LLM (Phi-3.5) detected.\n",
            "  [System] Seeding basic knowledge...\n",
            "  [Memory] Learned: bird --[can]--> fly\n",
            "  [Memory] Learned: fish --[can]--> swim\n",
            "  [Memory] Learned: human --[is]--> mortal\n",
            "  [Memory] Learned: dog --[can]--> bark\n",
            "  [Memory] Learned: penguin --[is]--> bird\n",
            "  [Memory] Learned: penguin --[cannot]--> fly\n",
            "\n",
            "ðŸ¤– ORIAN v56.0 (Oracle Sanitizer)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}